{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "# Initialize MLFlow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update with your MLFlow server URI\n",
    "mlflow.set_experiment(\"Sales_Forecast_Monitoring\")\n",
    "\n",
    "print(\"MLFlow version:\", mlflow.__version__)\n",
    "print(\"MLFlow tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your production model from MLFlow\n",
    "MODEL_URI = \"models:/sales_forecast/production\"  # Update with your model URI\n",
    "\n",
    "try:\n",
    "    model = mlflow.pyfunc.load_model(MODEL_URI)\n",
    "    print(f\"Successfully loaded model from {MODEL_URI}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_baseline(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Calculate and log baseline statistics for monitoring\n",
    "    \n",
    "    Args:\n",
    "        model: Your trained model\n",
    "        X_train: Training features DataFrame\n",
    "        y_train: Training target values\n",
    "        \n",
    "    Returns:\n",
    "        run_id: The MLFlow run ID where baseline was logged\n",
    "    \"\"\"\n",
    "    # Make predictions on training data\n",
    "    train_pred = model.predict(X_train)\n",
    "    \n",
    "    # Calculate baseline statistics\n",
    "    baseline_stats = {\n",
    "        'performance': {\n",
    "            'mae': mean_absolute_error(y_train, train_pred),\n",
    "            'rmse': mean_squared_error(y_train, train_pred, squared=False)\n",
    "        },\n",
    "        'features': {},\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Store feature distributions (sampling to avoid memory issues)\n",
    "    for col in X_train.columns:\n",
    "        baseline_stats['features'][col] = {\n",
    "            'mean': float(X_train[col].mean()),\n",
    "            'std': float(X_train[col].std()),\n",
    "            'sample_values': X_train[col].sample(min(1000, len(X_train))).values.tolist()\n",
    "        }\n",
    "    \n",
    "    # Log to MLFlow\n",
    "    with mlflow.start_run(run_name=\"baseline_establishment\") as run:\n",
    "        mlflow.log_dict(baseline_stats, \"baseline_stats.json\")\n",
    "        mlflow.log_metrics(baseline_stats['performance'])\n",
    "        mlflow.log_param(\"baseline_data_shape\", f\"{X_train.shape[0]} rows, {X_train.shape[1]} features\")\n",
    "        \n",
    "    print(f\"Baseline established in run {run.info.run_id}\")\n",
    "    return run.info.run_id, baseline_stats\n",
    "\n",
    "# Example usage (uncomment when ready):\n",
    "# baseline_run_id, baseline_stats = establish_baseline(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesForecastMonitor:\n",
    "    def __init__(self, model_uri, baseline_stats):\n",
    "        self.model = mlflow.pyfunc.load_model(model_uri)\n",
    "        self.baseline_stats = baseline_stats\n",
    "        \n",
    "    def check_feature_drift(self, new_data):\n",
    "        \"\"\"Detect feature drift using Kolmogorov-Smirnov test\"\"\"\n",
    "        drift_report = {}\n",
    "        for feature in self.baseline_stats['features']:\n",
    "            if feature in new_data.columns:\n",
    "                # KS test for continuous features\n",
    "                _, p_value = stats.ks_2samp(\n",
    "                    self.baseline_stats['features'][feature]['sample_values'],\n",
    "                    new_data[feature].dropna().values\n",
    "                )\n",
    "                drift_report[feature] = {\n",
    "                    'p_value': float(p_value),\n",
    "                    'drift_detected': p_value < 0.05,\n",
    "                    'baseline_mean': self.baseline_stats['features'][feature]['mean'],\n",
    "                    'current_mean': float(new_data[feature].mean())\n",
    "                }\n",
    "            else:\n",
    "                drift_report[feature] = {\n",
    "                    'error': 'Feature missing in new data'\n",
    "                }\n",
    "        return drift_report\n",
    "    \n",
    "    def check_performance_drift(self, y_true, y_pred):\n",
    "        \"\"\"Compare current performance with baseline\"\"\"\n",
    "        baseline_mae = self.baseline_stats['performance']['mae']\n",
    "        current_mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'metric': 'MAE',\n",
    "            'baseline_value': float(baseline_mae),\n",
    "            'current_value': float(current_mae),\n",
    "            'percent_change': float(((current_mae - baseline_mae) / baseline_mae) * 100),\n",
    "            'threshold_exceeded': abs((current_mae - baseline_mae) / baseline_mae) > 0.2  # 20% threshold\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monitoring_pipeline(model_uri, new_data, actuals=None, baseline_run_id=None):\n",
    "    \"\"\"\n",
    "    Complete monitoring pipeline for a single execution\n",
    "    \n",
    "    Args:\n",
    "        model_uri: URI of the model to monitor\n",
    "        new_data: DataFrame with new inference data\n",
    "        actuals: Optional array of true values (if available)\n",
    "        baseline_run_id: Run ID containing baseline stats\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predictions, drift_report, performance_report)\n",
    "    \"\"\"\n",
    "    # Load baseline stats if not provided\n",
    "    if baseline_run_id is None:\n",
    "        baseline_run_id = mlflow.search_runs(filter_string=\"tags.mlflow.runName='baseline_establishment'\").iloc[0].run_id\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"monitoring_run\", nested=True) as run:\n",
    "        # Load baseline data\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        baseline_stats = mlflow.artifacts.load_dict(f\"runs:/{baseline_run_id}/baseline_stats.json\")\n",
    "        \n",
    "        # Initialize monitor\n",
    "        monitor = SalesForecastMonitor(model_uri, baseline_stats)\n",
    "        \n",
    "        # Log monitoring metadata\n",
    "        mlflow.log_param(\"monitoring_timestamp\", datetime.now().isoformat())\n",
    "        mlflow.log_param(\"data_shape\", f\"{new_data.shape[0]} rows, {new_data.shape[1]} features\")\n",
    "        \n",
    "        # Check feature drift\n",
    "        drift_report = monitor.check_feature_drift(new_data)\n",
    "        mlflow.log_dict(drift_report, \"feature_drift_report.json\")\n",
    "        \n",
    "        # Count drifting features\n",
    "        drifting_features = sum(1 for f in drift_report.values() if isinstance(f, dict) and f.get('drift_detected', False))\n",
    "        mlflow.log_metric(\"drifting_features_count\", drifting_features)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = monitor.model.predict(new_data)\n",
    "        \n",
    "        # Check performance if actuals available\n",
    "        performance_report = None\n",
    "        if actuals is not None:\n",
    "            performance_report = monitor.check_performance_drift(actuals, predictions)\n",
    "            mlflow.log_metrics({\n",
    "                f\"current_{performance_report['metric']}\": performance_report['current_value'],\n",
    "                f\"{performance_report['metric']}_percent_change\": performance_report['percent_change']\n",
    "            })\n",
    "            \n",
    "            if performance_report['threshold_exceeded']:\n",
    "                mlflow.set_tag(\"alert\", \"performance_drift_detected\")\n",
    "        \n",
    "        return predictions, drift_report, performance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_performance_alert(run_id, recipients):\n",
    "    \"\"\"Generate and send performance alert email\"\"\"\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    \n",
    "    # Extract relevant information\n",
    "    metrics = run.data.metrics\n",
    "    params = run.data.params\n",
    "    \n",
    "    # Create alert message\n",
    "    alert_msg = f\"\"\"\n",
    "    Sales Forecast Model Alert\n",
    "    -------------------------\n",
    "    Run ID: {run_id}\n",
    "    Time: {params.get('monitoring_timestamp', 'N/A')}\n",
    "    \n",
    "    Performance Metrics:\n",
    "    - Current MAE: {metrics.get('current_MAE', 'N/A')}\n",
    "    - MAE Change: {metrics.get('MAE_percent_change', 'N/A')}%\n",
    "    \n",
    "    Feature Drift:\n",
    "    - Drifting Features: {metrics.get('drifting_features_count', 0)}\n",
    "    \n",
    "    Model Info:\n",
    "    - Data Processed: {params.get('data_shape', 'N/A')}\n",
    "    \n",
    "    View in MLFlow: {mlflow.get_tracking_uri()}/#/experiments/{run.info.experiment_id}/runs/{run_id}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send email (configure with your SMTP settings)\n",
    "    msg = MIMEText(alert_msg)\n",
    "    msg['Subject'] = \"ALERT: Sales Forecast Model Drift Detected\"\n",
    "    msg['From'] = \"ml-monitoring@yourcompany.com\"\n",
    "    msg['To'] = \", \".join(recipients)\n",
    "    \n",
    "    try:\n",
    "        with smtplib.SMTP('your.smtp.server', 587) as server:\n",
    "            server.starttls()\n",
    "            server.login(\"your_username\", \"your_password\")\n",
    "            server.send_message(msg)\n",
    "        print(\"Alert sent successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send alert: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_scheduled_monitoring(model_uri, data_loader_func, recipients):\n",
    "    \"\"\"Set up scheduled monitoring job\"\"\"\n",
    "    \n",
    "    def monitoring_job():\n",
    "        print(f\"Running scheduled monitoring at {datetime.now()}\")\n",
    "        try:\n",
    "            new_data, actuals = data_loader_func()\n",
    "            with mlflow.start_run(run_name=\"scheduled_monitoring\") as run:\n",
    "                _, _, performance_report = run_monitoring_pipeline(\n",
    "                    model_uri=model_uri,\n",
    "                    new_data=new_data,\n",
    "                    actuals=actuals\n",
    "                )\n",
    "                \n",
    "                if performance_report and performance_report['threshold_exceeded']:\n",
    "                    send_performance_alert(run.info.run_id, recipients)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Monitoring job failed: {str(e)}\")\n",
    "    \n",
    "    # Set up scheduler (runs daily at 2AM)\n",
    "    scheduler = BackgroundScheduler()\n",
    "    scheduler.add_job(monitoring_job, 'cron', hour=2)\n",
    "    scheduler.start()\n",
    "    print(\"Scheduled monitoring initialized - will run daily at 2AM\")\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "# Example usage:\n",
    "# def load_monitoring_data():\n",
    "#     new_data = pd.read_csv(\"new_sales_data.csv\")\n",
    "#     actuals = pd.read_csv(\"actuals.csv\")['sales'] if exists else None\n",
    "#     return new_data, actuals\n",
    "# \n",
    "# scheduler = setup_scheduled_monitoring(\n",
    "#     model_uri=MODEL_URI,\n",
    "#     data_loader_func=load_monitoring_data,\n",
    "#     recipients=[\"data-team@yourcompany.com\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example end-to-end usage (adapt to your specific case)\n",
    "\n",
    "# 1. Load your training data to establish baseline\n",
    "# X_train, y_train = load_training_data()\n",
    "# baseline_run_id, baseline_stats = establish_baseline(model, X_train, y_train)\n",
    "\n",
    "# 2. Load new data for monitoring\n",
    "# new_data = pd.read_csv(\"new_sales_data.csv\")\n",
    "# actuals = pd.read_csv(\"actuals.csv\")['sales']  # Optional if available\n",
    "\n",
    "# 3. Run monitoring\n",
    "# predictions, drift_report, perf_report = run_monitoring_pipeline(\n",
    "#     model_uri=MODEL_URI,\n",
    "#     new_data=new_data,\n",
    "#     actuals=actuals,\n",
    "#     baseline_run_id=baseline_run_id\n",
    "# )\n",
    "\n",
    "# 4. View results\n",
    "# print(\"Drift Report:\", json.dumps(drift_report, indent=2))\n",
    "# print(\"Performance Report:\", json.dumps(perf_report, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
